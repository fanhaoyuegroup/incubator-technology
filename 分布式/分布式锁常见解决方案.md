### 为什么需要分布式锁？

分布式系统中，同一系统的不同主机共享同一资源，在访问的时候需要添加互斥语义保证数据一致性。这种情况下就需要使用分布式锁。

### 基于数据库实现

```sql
create table TDistributedLock (
  `id` int(11) NOT NULL AUTO_INCREMENT COMMENT ' 主键 ',
  `lock_key` varchar(64) NOT NULL DEFAULT '' COMMENT ' 锁的键值 ',
  `lock_timeout` datetime NOT NULL DEFAULT NOW() COMMENT ' 锁的超时时间 ',
  `create_time` datetime NOT NULL DEFAULT NOW() COMMENT ' 记录创建时间 ',
  `modify_time` datetime NOT NULL DEFAULT NOW() COMMENT ' 记录修改时间 ',
  PRIMARY KEY(`id`),
  UNIQUE KEY `uidx_lock_key` (`lock_key`) USING BTREE
) ENGINE=InnoDB DEFAULT CHARSET=utf8 COMMENT=' 分布式锁表 ';
```

这样在需要加锁时，只需要往这张表里插入一条数据就可以了：

```sql
INSERT INTO TDistributedLock(lock_key, lock_timeout) values('lock_xxxx', '2020-07-19 18:20:00');
```

当对共享资源的操作完毕后，可以释放锁：

```sql
DELETE FROM TDistributedLock where lock_key='lock_xxxx';
```

该方案简单方便，主要利用数据库表的唯一索引约束特性，保证多个微服务模块同时申请分布式锁时，只有一个能够获得锁。

一些问题：

 - 获得锁的微服务模块意外 crash，来不及释放锁怎么办？
    - 因为在插入锁记录时，同时设置了一个 lock_timeout 属性，可以另外跑一个 lock_cleaner，将超时的锁记录删除。
 - 获得锁失败的微服务模块如何继续尝试获得锁？
    - 搞一个 while 循环，反复尝试，如能成功获得锁就跳出循环，否则 sleep 一会儿重新进入循环体继续尝试
 - 获得锁的微服务模块想重入地获得锁怎么办？
    - 在数据库表中加个字段，记录当前获得锁的机器的主机信息和线程信息，那么下次再获取锁的时候先查询数据库，如果当前机器的主机信息和线程信息在数据库可以查到的话，直接把锁分配给它就可以了。这时需要考虑业务的复杂程度，判断是否要延长锁的更新时间。
 - 数据库单点不安全怎么办？
    - 数据库领域有主从、一主多从、多主多从等复制方案，可保证一个数据库实例挂掉时，其它实例可以接管过来继续提供服务。不过需要注意的是有一些复制方案它是异步的，并不能保证写入一个数据库实例的数据立马可以在另外一个数据库实例中查询到， 这就容易造成锁丢失，导致授予了两把锁的问题。

### 基于 zookeeper 实现

zookeeper 是一种提供配置管理、分布式协同以及命名的中心化服务。很明显 zookeeper 本身就是为了分布式协同这个目标而生的，它采用一种被称为 ZAB(Zookeeper Atomic Broadcast) 的一致性协议来保证数据的一致性。基于 zk 的一些特性，我们很容易得出使用 zk 实现分布式锁的落地方案：

![](https://user-gold-cdn.xitu.io/2018/10/8/166513ddf9f2bd87?imageView2/0/w/1280/h/960/format/webp/ignore-error/1)

 1. 使用 zk 的临时节点和有序节点，每个线程获取锁就是在 zk 创建一个临时有序的节点，比如在/lock/目录下。
 2. 创建节点成功后，获取 /lock 目录下的所有临时节点，再判断当前线程创建的节点是否是所有的节点的序号最小的节点
 3. 如果当前线程创建的节点是所有节点序号最小的节点，则认为获取锁成功。
 4. 如果当前线程创建的节点不是所有节点序号最小的节点，则对节点序号的前一个节点添加一个事件监听。 比如当前线程获取到的节点序号为`/lock/003`,然后所有的节点列表为 `[/lock/001,/lock/002,/lock/003]`,则对 `/lock/002` 这个节点添加一个事件监听器。

如果锁释放了，会唤醒下一个序号的节点，然后重新执行第 3 步，判断是否自己的节点序号是最小。

比如`/lock/001`放了，`/lock/002` 监听到时间，此时节点集合为` [/lock/002,/lock/003]`,则 `/lock/002` 为最小序号节点，获取到锁。

从数据一致性的角度来说，zk 的方案无疑是最可靠的，而且等候锁的客户端也不用不停地轮循锁是否可用，当锁的状态发生变化时可以自动得到通知。

zookeeper 的缺陷：

 - 性能问题：zookeeper 作为分布式协调系统，不适合作为频繁的读写存储系统。而且通过增加 zookeeper 服务器来提高集群的读写能力也是有上限的，因为 zookeeper 集群规模越大，zookeeper 数据需要同步到更多的服务器。同时 zookeeper 分布式锁每一次都要申请锁和释放锁，都要动态创建删除临时节点，所以 zookeeper 不能维护大量的分布式锁，也不能维护大量的客户端心跳长连接。在分布式定理中，zookeeper 追求的是 CP，也就是 zookeeper 保证集群向外提供统一的视图，但是 zookeeper 牺牲了可用性，在极端情况下，zookeeper 可能会丢弃一些请求。并且 zookeeper 集群在进行 leader 选举的时候整个集群也是不可用的，集群选举时间长达 30 ~ 120s。
 - 惊群效应：在获取锁的时候有一个细节，客户端在获取锁失败的情况下，会监听/lock 节点。这会存在性能问题，如果在/lock 节点下排队等待的有 1000 个进程，那么锁持有者释放锁 (删除/lock 节点下的临时节点) 时，zookeeper 会通知监听/lock 的 1000 个进程。然后这 1000 个进程会读取 zookeeper 下/lock 节点的全部临时节点，然后判断自己是否为最小的临时节点。但是这 1000 个进程中只有一个最小序号的进程会持有分布式锁，也就是说 999 个进程做的都是无用功。这些无用功会对 zookeeper 造成较大压力的读负载。为了解决惊群效应，需要对 zookeeper 分布式锁监听逻辑进行优化，实际上，排队进程真正感兴趣的是比自己临时节点序号小的节点，我们只需要监听序号比自己小的节点。

为什么不使用 zookeeper 的持久节点？

当使用持久节点时如果加锁后出现异常，节点来不及删除，会导致后面的节点会一直等待节点删除，从而出现死锁，临时节点因为会随着客户端的下线被删除，可以避免死锁的问题。

### 基于 redis 实现

首先，Redis 客户端为了获取锁，向 Redis 节点发送如下命令：

```sql
SET resource_name my_random_value NX PX 30000
```
上面的命令如果执行成功，则客户端成功获取到了锁，接下来就可以访问共享资源了；而如果上面的命令执行失败，则说明获取锁失败。

`my_random_value` 是由客户端生成的一个随机字符串，它要保证在足够长的一段时间内在所有客户端的所有获取锁的请求中都是唯一的。

最后，当客户端完成了对共享资源的操作之后，执行下面的 Redis Lua 脚本来释放锁：

```lua
if redis.call("get",KEYS[1]) == ARGV[1] then
    return redis.call("del",KEYS[1])
else
    return 0
end
```

这段脚本在删除 key 之前会先获取到 value 值，判断是否与 `my_random_value` 相同，只有相同情况下才进行删除，通过 lua 脚本来保证删除操作的原子性。

一些问题：

一、这个锁必须要设置一个过期时间。否则的话，当一个客户端获取锁成功之后，假如它崩溃了，或者由于发生了网络分割（network partition）导致它再也无法和 Redis 节点通信了，那么它就会一直持有这个锁，而其它客户端永远无法获得锁了

二、第一步获取锁的操作一定要是原子性的，不可以分成如下两个命令

```sql
SETNX resource_name my_random_value
EXPIRE resource_name 30
```

虽然这两个命令和前面算法描述中的一个 SET 命令执行效果相同，但却不是原子的。如果客户端在执行完 SETNX 后崩溃了，那么就没有机会执行 EXPIRE 了，导致它一直持有这个锁。

三、设置一个随机字符串 my_random_value 是很有必要的，它保证了一个客户端释放的锁必须是自己持有的那个锁。假如获取锁时 SET 的不是一个随机字符串，而是一个固定值，那么可能会发生下面的执行序列：

 1. 客户端 1 获取锁成功。
 2. 客户端 1 在某个操作上阻塞了很长时间。
 3. 过期时间到了，锁自动释放了。
 4. 客户端 2 获取到了对应同一个资源的锁。
 5. 客户端 1 从阻塞中恢复过来，释放掉了客户端 2 持有的锁。

四、释放锁的操作必须使用 Lua 脚本来实现。释放锁其实包含三步操作：‘GET’、判断和‘DEL’，用 Lua 脚本来实现能保证这三步的原子性。

五、假如 Redis 节点宕机了，那么所有客户端就都无法获得锁了，服务变得不可用。为了提高可用性，我们可以给这个 Redis 节点挂一个 Slave，当 Master 节点不可用的时候，系统自动切到 Slave 上（failover）。但由于 Redis 的主从复制（replication）是异步的，这可能导致在 failover 过程中丧失锁的安全性。

针对上面第五个问题，redis 的作者设计了 Redlock 算法。Redlock 获取锁的流程如下：

 1. 获取当前时间（毫秒）。
 2. 按顺序依次向 N 个 Redis 节点执行获取锁的操作。这个获取操作跟前面基于单 Redis 节点的获取锁的过程相同，包含随机字符串 my_random_value，也包含过期时间 (即锁的有效时间)。为了保证在某个 Redis 节点不可用的时候算法能够继续运行，这个获取锁的操作还有一个超时时间 (time out)，它要远小于锁的有效时间（几十毫秒量级）。客户端在向某个 Redis 节点获取锁失败以后，应该立即尝试下一个 Redis 节点。这里的失败，应该包含任何类型的失败，比如该 Redis 节点不可用，或者该 Redis 节点上的锁已经被其它客户端持有（注：Redlock 原文中这里只提到了 Redis 节点不可用的情况，但也应该包含其它的失败情况）。
 3. 计算整个获取锁的过程总共消耗了多长时间，计算方法是用当前时间减去第 1 步记录的时间。如果客户端从大多数 Redis 节点（>= N/2+1）成功获取到了锁，并且获取锁总共消耗的时间没有超过锁的有效时间 (lock validity time)，那么这时客户端才认为最终获取锁成功；否则，认为最终获取锁失败。
 4. 如果最终获取锁成功了，那么这个锁的有效时间应该重新计算，它等于最初的锁的有效时间减去第 3 步计算出来的获取锁消耗的时间。
 5. 如果最终获取锁失败了（可能由于获取到锁的 Redis 节点个数少于 N/2+1，或者整个获取锁的过程消耗的时间超过了锁的最初有效时间），那么客户端应该立即向所有 Redis 节点发起释放锁的操作（即前面介绍的 Redis Lua 脚本）。

前面讨论的单 Redis 节点的分布式锁在 failover 的时候锁失效的问题，在 Redlock 中不存在了，但如果有节点发生崩溃重启，还是会对锁的安全性有影响的。

假设一共有 5 个 Redis 节点：A, B, C, D, E。设想发生了如下的事件序列：

 1. 客户端 1 成功锁住了 A, B, C，获取锁成功（但 D 和 E 没有锁住）。
 2. 节点 C 崩溃重启了，但客户端 1 在 C 上加的锁没有持久化下来，丢失了。
 3. 节点 C 重启后，客户端 2 锁住了 C, D, E，获取锁成功。

这样，客户端 1 和客户端 2 同时获得了锁。除了上面所述的问题，以下几种情况也可能导致 Redlock 失效：

 - 时钟发生跳跃。
 - 长时间的 GC pause。
 - 长时间的网络延迟。

### etcd 分布式锁实现

etcd 是 CoreOS 团队于 2013 年 6 月发起的开源项目，它的目标是构建一个高可用的分布式键值 (key-value) 数据库。etcd 内部采用 raft 协议作为一致性算法，etcd 基于 Go 语言实现。

etcd 分布式锁：

 - Lease 机制：即租约机制（TTL，Time To Live），etcd 可以为存储的 KV 对设置租约，当租约到期，KV 将失效删除；同时也支持续约，即 KeepAlive。
 - Revision 机制：每个 key 带有一个 Revision 属性值，etcd 每进行一次事务对应的全局 Revision 值都会加一，因此每个 key 对应的 Revision 属性值都是全局唯一的。通过比较 Revision 的大小就可以知道进行写操作的顺序。
 - 在实现分布式锁时，多个程序同时抢锁，根据 Revision 值大小依次获得锁，可以避免 “羊群效应” （也称 “惊群效应”），实现公平锁。
 - Prefix 机制：即前缀机制，也称目录机制。可以根据前缀（目录）获取该目录下所有的 key 及对应的属性（包括 key, value 以及 revision 等）。
 - Watch 机制：即监听机制，Watch 机制支持 Watch 某个固定的 key，也支持 Watch 一个目录（前缀机制），当被 Watch 的 key 或目录发生变化，客户端将收到通知。

### 参考

[微服务中的分布式锁方案 ](https://jeremyxu2010.github.io/2020/07/%E5%BE%AE%E6%9C%8D%E5%8A%A1%E4%B8%AD%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E9%94%81%E6%96%B9%E6%A1%88/)

[分布式锁 ](https://juejin.cn/post/6844903688088059912)

[基于 Redis 的分布式锁到底安全吗（上）？](http://zhangtielei.com/posts/blog-redlock-reasoning.html)

[基于 Redis 的分布式锁到底安全吗（下）？](http://zhangtielei.com/posts/blog-redlock-reasoning-part2.html)

[基于 etcd 实现分布式锁 ](https://segmentfault.com/a/1190000021603215)


