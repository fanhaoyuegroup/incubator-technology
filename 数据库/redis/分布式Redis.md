## 分布式Redis面临的问题
#### 1：数据量伸缩
单实例存储的Key-Value受限于单机的内存和磁盘的容量，虽然Redis提供了Key的过期机制，在作为缓存使用的时候可以通过淘汰过期的数据达到控制容量的目的，但当业务数据需要长期有效或者数据量确实很大的情况下的时候淘汰机制将不再适用。

#### 2：访问量伸缩
单实例Redis吞吐量受限于单词请求处理的平均时耗。业务数据吞吐量过高的时候需要相应提高redis的处理能力。

#### 3：单点故障。

当发生了不可恢复故障的时候如何恢复对应业务数据的可用性

#### 4：水平拆分
 分布式环境下，节点分为不同的分组对应不同的业务数据，通过分组，解决数据量的一个瓶颈，同时通过分摊请求到不同的分组，全局的吞吐量耶增加，访问量的伸缩性则可以解决。
 
#### 5：主备复制


同一份业务数据存在多个副本，根据数据的访问规则分发到某一个或多个副本执行，通过W+R>N的读写配置可以做到读取数据内容的实时，随着N的增加，当读写访问量差不多的时候，业务的吞吐量相比单实例会提升近2倍，当读的量大于写的量的时候，吞吐量会随随着读写比例的增加而提升

#### 6:故障转移

业务数据所在节点发生故障的时候，这部分业务数据转移到其他节点上进行。因此，为了支撑故障转移，需要业务数据保持多个副本，位于不同的节点。

## 具体的解决方案

#### 1:水平拆分
 解决数据量和访问量增加后单节点造成的性能压力，通常引入水平拆分机制，将数据存储和对应数据的访问分散到不同节点进行分别处理。水平拆分后的节点存储和处理的数据没有交集，使得节点之间相互独立，但内部拆封和多节点通常对外部服务透明，通过数据分布和路由请求的配合。可以做到数据存放访问对水平拆分的适配。
 
 
 水平拆分映射主要是三种方式：hash映射，范围映射，hash和范围结合。该三种方法属于数据库水平拆分的常用途径，不做赘述。
#### 2:请求路由
 跨实例的读请求：需要将多个Key分别分发到对应实例上执行，在合并结果。其中设计到语句的拆分和重生成。
 
 跨实例的原子读写请求：事务集合型数据的转存操作，像实例B的写入操作依赖与对实例A的读取，单实例的情况下保证此类读写依赖的并发安全，然而跨实例的情况下，跨节点读写依赖的原子请求是不支持的。
 
     在RedisCluster之前，通常通过proxy代理层处理sharding的逻辑。
    
## 主备复制

主备复制需要考虑的核心就是数据的一致性。凡是牵扯到备机的都存在该问题。Redis采用的是主备复制的方式保证一致性。（存在延迟），主节点即Master一般对外提供写服务，之后Redis内部异步的将数据从主节点复制到其他节点（slave）。

#### 主备复制流程
    
    master节点对外提供写服务，slave节点作为master的数据备份，拥有master的全量数据，对外不提供写服务，主备复制由slave节点触发。
1:首先slave启动后向master发起SYNC命令，master被动的将slave节点加入自己的主备复制集群。

2:master收到SYNC命令，开启BGSAVE操作（Redis的全量持久化操作）

3:BGSAVE完成后，master将快照信息发送给slave

4:发送期间，master收到新的写操作，会再存一份到backlog队列。

5:快照信息发送完毕，master将继续发送backlog队列信息

6:backlog发送完成后，后续的写操作同时发给slave，保证实时的异步复制

    如果有多个slave节点发生sync命令，只要在master完成BGSAVE之前，都将收到一份快照信息，否则就触发master第二次BGSAVE。
    
#### 断点续传

   全量复制的过程中出现意外导致数据出现中断，Redis的PSYNC可用以取代SYNC，做到master基于断点续传的主备同步协议。master-slave两端通过维护一个offset记录当前已经同步过的命令，slave挂了之后，master的客户端命令会保持在缓存中，在slave重连后，告知master断开时候的offset，master将缓存中大于offset的数据发送给slave
   
## 故障转移
   
   
 
    故障转移即master挂掉之后我们如何能够第一时间切换至从机，master在未恢复之前被驱逐，其余的slave参与master的竞选工作，选举出新的master。所以核心问题就在于如何监听master挂了。
    
    凡是集群都存在故障转移，其方法都可作为参照。
    
#### sentinel
我们一般会这样做，保持相应的daemon进程用作监听，多个daemon组成了一个sentinel集群，这些节点相互通信选举协商，在master节点的故障发现故障转移决策上表现一致。

   所有具有监听相同master的节点都需要和其他sentinel节点保持相互感知，他们订阅相同的master节点上相同的channel，新加入的sentinel节点向这个channel发布一条信息，包含自己的信息，改channel的订阅者们就可以发现这个新的sentinel接待您，随后，sentinel节点之间建立长链接。
   
#### 故障发现

sentinel节点定期和master进行心跳检测，一旦发现master未正确相应，就认为他挂了，所以该节点会将master置为主观不可用状态。随后他会和其他节点进行确认，当确认的sentinel节点数>=quorum(可配置)，则认为确实挂了，即客观不可用状态。然后进入failover，选举新的master。

#### failover决策

Redis的sentinel机制采用类似Raft协议实现选举算法。

1:sentinelState的epoch变量类似与raft协议中的trem（选举回合）

2:每一个确认了master客观不可用状态的sentinel节点会向周围广播自己参选的请求。

3:每一个接收到参选请求的sentinel节点如果还没有给请他人表示过自己要参选的请求，那么他就将本选举回合的意向置为首个参选sentinel并回复他。如果已经表示过意向了，那么本回合拒绝所有其他的参选请求，并将已有意向回复给参选的sentinel

4:每一个参选的sentinel节点如果超过了一半的意向统计某个参选sentinel（亦可以是自己）则确定该sentinel为新的leader，如果本回合持续了足够长的释迦还未选出leader，则开启下一回合。

5:新的leader确定之后，从之前master下的从机依据一定的规则选取一个slave为新的master，并告知其他slave此为新的master，都来连接这个master。
